{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library BeautifulSoup is requird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T15:07:46.237415Z",
     "start_time": "2021-08-18T15:07:46.232213Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T15:07:46.276694Z",
     "start_time": "2021-08-18T15:07:46.270127Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def conditionalMkdir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T15:07:46.516529Z",
     "start_time": "2021-08-18T15:07:46.297749Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import urllib.request\n",
    "import re\n",
    "import json\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "try:\n",
    "        from BeautifulSoup import BeautifulSoup\n",
    "except ImportError:\n",
    "        from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "''' Finds all links to XPT files in source HTML '''\n",
    "def parsePageXPT(html_source):\n",
    "    # Parse HTML source code with BeautifulSoup library\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "    # Get all <a>...</a> with .XPT extensions\n",
    "    xpt_urls = soup.findAll('a', href=re.compile('\\.XPT$'))\n",
    "    xpt_urls = [url['href'] for url in xpt_urls]\n",
    "    return xpt_urls\n",
    "\n",
    "\n",
    "''' Parses a page for 'codebook' div with descriptions of column labels '''\n",
    "def parsePageLabels(html_source):\n",
    "    # Parse HTML source code with BeautifulSoup library\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "    # Find div element with codebook\n",
    "    try:\n",
    "        div = soup.findAll('div', id=\"CodebookLinks\")[0]\n",
    "    except:\n",
    "        # If we can't find the div, print\n",
    "        print('Error, no CodebookLinks Div')\n",
    "        return {}\n",
    "\n",
    "    # Get all links and their text in the div\n",
    "    labels = [link.string.rstrip() for link in div.findAll('a')]\n",
    "\n",
    "    # Put labels into library\n",
    "    labels = [re.split('( - )', label, 1) for label in labels]\n",
    "    labels = {label[0]:label[-1] for label in labels}\n",
    "    return labels\n",
    "\n",
    "\n",
    "'''Get year associated with file '''\n",
    "def getFileYear(file_url):\n",
    "    # Search URL for a year\n",
    "    year = re.search('\\/(\\d+-\\d+)\\/', file_url)\n",
    "\n",
    "    # Get value from regular expression match\n",
    "    if year:\n",
    "        year = year.group(1)\n",
    "    else:\n",
    "        # If no match, assign year as 'Other'\n",
    "        year = 'Other'\n",
    "\n",
    "    return year\n",
    "\n",
    "\n",
    "''' Creates directory for file and downloads file from provided URL '''\n",
    "def getFile(file_dir, file_url, file_type):\n",
    "    # Get data year\n",
    "    year = getFileYear(file_url)\n",
    "#     if year == '2017-2018':\n",
    "    # Compile file location\n",
    "    file_dir = os.path.join(file_dir, year, file_type)\n",
    "\n",
    "    # Make directory for file if necessary\n",
    "    conditionalMkdir(file_dir)\n",
    "\n",
    "    # Get name for file\n",
    "    file_name = file_url.split('/')[-1]\n",
    "    file_loc = os.path.join(file_dir, file_name)\n",
    "\n",
    "    # Check that file does not already exist\n",
    "    if not os.path.isfile(file_loc):\n",
    "        print('Getting file: %s' % file_url)\n",
    "        # Download the file and write to local\n",
    "\n",
    "        urllib.request.urlretrieve(file_url, file_loc)\n",
    "#         print('sleep')\n",
    "#         time.sleep(5)\n",
    "\n",
    "\n",
    "''' Obtains column labels from NHANES website and saves to JSON '''\n",
    "def getLabel(file_dir, file_url, file_type):\n",
    "    # Get data year\n",
    "    year = getFileYear(file_url)\n",
    "\n",
    "    # Combile file location:\n",
    "    file_dir = os.path.join(file_dir, year, file_type)\n",
    "\n",
    "    # Get name for file\n",
    "    file_name = file_url.split('/')[-1].replace('.XPT', '.JSON')\n",
    "    file_loc = os.path.join(file_dir, file_name)\n",
    "\n",
    "    # Modify XPT file_url to load page with labels\n",
    "    file_url = file_url.replace('.XPT', '.htm')\n",
    "\n",
    "    # Check that file does not already exist\n",
    "    if not os.path.isfile(file_loc):\n",
    "        # Open the website and download source HTML\n",
    "        with urllib.request.urlopen(file_url) as page:\n",
    "            html_source = page.read()\n",
    "\n",
    "        # Parse the website for column label\n",
    "        file_labels = parsePageLabels(html_source)\n",
    "\n",
    "        # Save the file to JSON\n",
    "        print('Saving label data: %s' % file_loc)\n",
    "        with open(file_loc, 'w') as open_file:\n",
    "            json.dump(file_labels, open_file)\n",
    "\n",
    "\n",
    "''' Reads HTML source from provided URLs, parses HTML for XPT files, and saves files '''\n",
    "def parseWebSite(url, output_dir):\n",
    "    # Get base URL for appending to relative file URLs\n",
    "    base_url = 'http://' + url.lstrip('http://').split('/')[0]\n",
    "\n",
    "    # Get file type for this URL\n",
    "    file_type = re.search('Component=([a-zA-Z]+)', url)\n",
    "    if file_type:\n",
    "        file_type = file_type.group(1)\n",
    "    else:\n",
    "        file_type = 'Other'\n",
    "\n",
    "    # Open the website and download source HTML\n",
    "    with urllib.request.urlopen(url) as page:\n",
    "        html_source = page.read()\n",
    "\n",
    "    # Parse the website for .XPT file links\n",
    "    file_urls = parsePageXPT(html_source)\n",
    "    file_urls = [base_url + file_url for file_url in file_urls]\n",
    "\n",
    "    # Download each file and store locally\n",
    "    for file_url in file_urls:\n",
    "        getFile(output_dir, file_url, file_type)\n",
    "        getLabel(output_dir, file_url, file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T15:07:47.280817Z",
     "start_time": "2021-08-18T15:07:47.261544Z"
    }
   },
   "outputs": [],
   "source": [
    "url = ['http://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics',\n",
    "'http://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Dietary',\n",
    "'http://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination',\n",
    "'http://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory',\n",
    "'http://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Questionnaire',\n",
    "'http://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Non-Public']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T07:21:58.534115Z",
     "start_time": "2021-09-03T07:21:58.527631Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Demographics\n",
    "output_dir = './data/raw_data/'\n",
    "parseWebSite(url[0], output_dir)\n",
    "\n",
    "#  Dietary\n",
    "output_dir = './data/raw_data/'\n",
    "parseWebSite(url[1], output_dir)\n",
    "\n",
    "#  Examination\n",
    "output_dir = './data/raw_data/'\n",
    "parseWebSite(url[2], output_dir)\n",
    "\n",
    "#  Laboratory\n",
    "output_dir = './data/raw_data/'\n",
    "parseWebSite(url[3], output_dir)\n",
    "\n",
    "#  Questionnaire\n",
    "output_dir = './data/raw_data/'\n",
    "parseWebSite(url[4], output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
